import numpy as np
import jax
import jax.numpy as jnp
import jax.tree_util as jtu
from jax import random, vmap, lax, grad
from jifty import FourierFeaturesLayer, MLPModified
import blackjax
import blackjax.sgmcmc.gradients as gradients
import functools as ft
from functools import partial
import equinox as eqx
import pickle
import os
from config import *

rng_key=jax.random.PRNGKey(0)
data_spatial_points,data_time_values,data_mag_values,data_phase_values,sigma_mag,sigma_phase_x,sigma_phase_y,sigma_phase_z,num_obs=load_data()
dataset=CombinedTimeStepDataset(data_spatial_points, data_mag_values, data_phase_values, data_time_values)

def psgld_step(rng_key, position, batch, step_size, grad_estimator=grad_estimator):
    
    # Compute the stochastic gradient
    grad = grad_estimator(position,batch)
    G=compute_preconditioner(grad)
    # Apply preconditioner to the gradient
    preconditioned_grad = jax.tree_map(lambda g, pre: g * pre, grad, G)
    
    # Update position with preconditioned gradient
    noise = jax.tree_map(lambda g: jax.random.normal(rng_key, shape=g.shape) * jnp.sqrt(step_size * g), G)
    new_position = jax.tree_map(lambda pos, grad, noise: pos - (step_size / 2) * grad + noise,
                                  position, preconditioned_grad, noise)
    
    return new_position


def compute_preconditioner(grad, epsilon=1e-8):
    # Square the gradient element-wise
    grad_squared = jax.tree_map(jnp.square, grad)
    
    # Add epsilon for numerical stability
    grad_plus_epsilon = jax.tree_map(lambda x: x + epsilon, grad_squared)
    # Take the reciprocal of the square root element-wise
    preconditioner = jax.tree_map(lambda x: 1.0 / jnp.sqrt(x), grad_plus_epsilon)
    
    # This will be a vector representing the diagonal of the preconditioning matrix G
    return preconditioner

def psgld_sampling(theta_init,
                batch_size,
                total_samples, thinning_factor, lr_package,
                rng_key):
    
    a1, a2, c = lr_package

  
    sgld = blackjax.sgld(grad_estimator)
    position = sgld.init(theta_init)
    psgld_step_jit= eqx.filter_jit(psgld_step)
    #Executes one step of the SGLD algirithm 
    @eqx.filter_jit
    def psgld_step_fun(iter,carry):
        """One step of SGLD"""
        position, rng_key = carry
        rng_key, batch_key, sample_key = jax.random.split(rng_key, 3)

        data_batch=get_batch(dataset,batch_size,batch_key)
        lr = lr_schedule(iter, a1, a2, c)
        new_position=psgld_step_jit(sample_key, position, data_batch, lr)
        return (new_position, rng_key)
    
    #Executes thinning_factor iterations and returns the last sample
    def last_sample(current_iter,init):
        val=jax.lax.fori_loop(current_iter,int(current_iter+thinning_factor),psgld_step_fun,init)
        sample,rng_key=val
        return current_iter+thinning_factor,sample,rng_key
    
    
    current_iter = 0
    init = (position, rng_key)
    if not os.path.exists("runs/Preconditioned-SGLD"):
        os.makedirs("runs/Preconditioned-SGLD")

    if thinning_factor==1:
        if not os.path.exists("runs/Preconditioned-SGLD/models_no_thinning"):
            os.makedirs("runs/Preconditioned-SGLD/models_no_thinning")
    else:
        if not os.path.exists("runs/Preconditioned-SGLD/models"):
                os.makedirs("runs/Preconditioned-SGLD/models")

    order=1
    no_thinning=thinning_factor==1
    
    for i in range(total_samples):
        iter, new_sample, rng_key = last_sample(current_iter, init)
        if no_thinning:
            eqx.tree_serialise_leaves(f"test/PSGLD_sampling_{order}_batch_size_{batch_size}_thinning_{thinning_factor}.eqx",new_sample) 
        else:
            eqx.tree_serialise_leaves(f"test/PSGLD_sampling_{order}_batch_size_{batch_size}_thinning_{thinning_factor}.eqx",new_sample)
        current_iter = iter
        init = (new_sample, rng_key)
        order+=1

    

#We define the learning rate package that dicatetes the learning rate at each step 
#and the initial position. User may chaing name of file (make sure the saved pytree has 
#the same structure as the initial model, meaning params_init), as well as the other parametres
#of the sampling.
def main():
    lr_package = (0.01, 0.51, 0.001)
    theta_init=eqx.filter(params_init,eqx.is_array)
    batch_size=10
    total_samples=10 
    thinning=5
    psgld_sampling(theta_init,batch_size,total_samples,thinning,lr_package,rng_key)

    

if __name__=="__main__":
    main()
